{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem flatten 1\n",
    "\n",
    "We have this input:\n",
    "\n",
    "<code>\n",
    "{\n",
    "   \"ProductNum\":\"6000078\",\n",
    "   \"Properties\":[\n",
    "      {\n",
    "         \"key\":\"invoice_id\",\n",
    "         \"value\":\"923659\"\n",
    "      },\n",
    "      {\n",
    "         \"key\":\"job_id\",\n",
    "         \"value\":\"296160\"\n",
    "      },\n",
    "      {\n",
    "         \"key\":\"sku_id\",\n",
    "         \"value\":\"312002\"\n",
    "      }\n",
    "   ],\n",
    "   \"unitCount\":\"3\"\n",
    "}\n",
    "</code>\n",
    "  \n",
    " And we want to flatten this into:\n",
    "\n",
    "<code> \n",
    "+-------------------------------------------------------+   \n",
    "| ProductNum | invoice_id | job_id | sku_id | unitCount |  \n",
    "+-------------------------------------------------------+   \n",
    "| 6000078    | 923659     | 296160 | 312002 | 3         |  \n",
    "+-------------------------------------------------------+\n",
    "<code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the json into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql.functions import udf, col, explode\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "from pyspark.sql import Row\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
    "\n",
    "DS_Products = \"\"\"{\n",
    "   \"ProductNum\":\"6000078\",\n",
    "   \"Properties\":[\n",
    "      {\n",
    "         \"key\":\"invoice_id\",\n",
    "         \"value\":\"923659\"\n",
    "      },\n",
    "      {\n",
    "         \"key\":\"job_id\",\n",
    "         \"value\":\"296160\"\n",
    "      },\n",
    "      {\n",
    "         \"key\":\"sku_id\",\n",
    "         \"value\":\"312002\"\n",
    "      }\n",
    "   ],\n",
    "   \"UnitCount\":\"3\"\n",
    "}\"\"\"\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "rddJson = spark.sparkContext.parallelize([DS_Products])\n",
    "DF_Products = spark.read.json(rddJson)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten this json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|ProductNum|UnitCount|          SubContent|\n",
      "+----------+---------+--------------------+\n",
      "|   6000078|        3|{invoice_id, 923659}|\n",
      "|   6000078|        3|    {job_id, 296160}|\n",
      "|   6000078|        3|    {sku_id, 312002}|\n",
      "+----------+---------+--------------------+\n",
      "\n",
      "+----------+---------+----------+------+------+\n",
      "|ProductNum|UnitCount|invoice_id|job_id|sku_id|\n",
      "+----------+---------+----------+------+------+\n",
      "|   6000078|        3|    923659|296160|312002|\n",
      "+----------+---------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flatten = DF_Products\\\n",
    "  .select(\"*\", explode(\"Properties\").alias(\"SubContent\"))\\\n",
    "  .drop(\"Properties\")\n",
    " \n",
    "df_flatten.show()\n",
    " \n",
    "df_flatten_pivot = df_flatten\\\n",
    "  .groupBy(\"ProductNum\",\"UnitCount\")\\\n",
    "  .pivot(\"SubContent.key\")\\\n",
    "  .agg(first(\"SubContent.value\"))\n",
    " \n",
    "df_flatten_pivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about aggregates functions here:\n",
    "https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
